## Against Justaism

This repository accompanies the paper:

```
@article{againstJustaism,
  title={Against Justaism: A call for more measured discussions on LLM cognition},
  author={Zak Hussain and Rui Mata and Dirk U. Wulff},
  journal={arXiv},
  year={2024}
  url={https://arxiv.org/XX}
}
```
It collects primary and secondary references to "Justaism" (pronounced "just-a-ism"), a term [coined](https://scottaaronson.blog/?p=7784) by Scott Aaronson (pejoratively) 
to refers to the deflationary claims made by skeptics that LLMs are "just...": "next-token predictors", "function approximators", or "stochastic parrots", and thus lack some
essential cognitive capacities that humans possess.

**We invite you to 
<a href="https://github.com/Zak-Hussain/againstJustaism/issues/new/choose">contribute to this repository</a> by submitting new instances of Justaism that you come across in the literature or in the media.**

### Primary instances of Justaism

"The widespread enthusiasm for LLMs should be tempered by an awareness that they are not actually simulating human intelligence [29,32,33]. LLMs simply predict the next phrase or sentence, given what they have been exposed to in the training data. Consequently, they tend to output an ‘average’ of what the internet or popular books tend to say"<br>
[Demszky et al., 2023, Nature Reviews Psychology](https://doi.org/10.1038/s44159-023-00241-5)

"A bare-bones LLM does not really know anything because all it does, at a fundamental level, is sequence prediction."<br>
[Shanahan, 2024, Communications of the ACM](https://dx.doi.org/10.1145/3624724)

"As sequence predictors, these models draw on the underlying statistical distribution of previously generated text to stitch together vectorized symbol strings based on the probabilities of their co-occurrence4. They therefore lack the communicatively embodied and relational functionings that are a prerequisite of scientific meaning-making, in the barest sense."<br>
[Birhane et al., 2023, Nature Reviews Physics](https://doi.org/10.1038/s42254-023-00581-4)

"LLMs are not trained to understand language as humans do. By ‘learning’ the statistical associations between words as they have been used by humans, GPT-3 develops an ability to successfully predict which word best completes a phrase or sentence."<br>
[Thirunavukarasu et al., 2023, Nature Medicine](https://doi.org/10.1038/s41591-023-02448-8)

"In our view, they [LLMs] operate over ‘fossilized’ outputs of human language (text token) and seem capable of implementing some automatic computations pertaining to distributional statistics, but are incapable of understanding due to their lack of generative world models"<br>
[Marcus et al., 2023, arXiv](https://doi.org/10.48550/arXiv.2308.00109)

"An AI doesn’t contemplate your question. When you understand how they work, even at a superficial level, you realize it’s just a statistical algorithm. It’s a very cool and impressive statistical algorithm, but it doesn’t think or consider. Some people are surprised to learn that there’s no ‘mind’ there at all."<br>
[Woolridge, 2023, Interview with Elsevier Connect](https://www.elsevier.com/connect/with-the-rise-of-llms-what-should-we-really-be-concerned-about)

"First we should ask the question, have large language models achieved anything, anything in this domain [natural language understanding]. Answer: no, they've achieved zero. [...] GPT-3 has done nothing. With a supercomputer it can look at 45 terabytes of data and find some superficial regularities, which then, it can imitate. "<br>
[Chomsky, 2023, Interview with Machine Learning Street Talk](https://www.youtube.com/watch?v=axuGfh4UR9Q)

"Our learned ability to process and find meaning in conversational text
means that we slip, and often forget that we are not dealing with a human, and
very quickly we anthropomorphise the bot. We may attribute personality traits
to it. We may attribute an intended meaning to the generated text. Anthropomorphism of this nature is not trivial and can have serious consensuses, indeed
previous research has highlighted people’s tendency to “overtrust” robotic systems with potentially disastrous outcomes [50].." <br>
[O'Neil & Connor, 2023, arXiv](https://doi.org/10.48550/arXiv.2307.04821)


### Secondary references to Justaism 

In response to the survey item – "Some generative model trained only on text, given enough data and computational resources, could understand natural
language in some non-trivial sense." – 51% agreed and the remaining 49% disagreed (out of 480 active NLP researcher respondents).<br>
[Michael et al., 2022, arXiv](https://doi.org/10.48550/arXiv.2208.12852)

'there is a small but growing belief that a next-token predicting model is merelyan impressive improv artist that cannot truly model human thought. [...] This criticism has been floating around as an informal viewpoint (LeCun, 2024; Bubeck et al., 2023).'<br>
[Bachmann & Nagarajan, 2024, arXiv](https://doi.org/10.48550/arXiv.2403.06963) 

"I suspect that the general public, including many journalists reporting on machine learning, aren’t even aware of the distinction between training the model and using it to make inferences. One simply reads that ChatGPT, or any other comparable LLM, generates text by predicting the next word. This mis-communication is a MAJOR blunder." <br>
[Benzon, 2023, LessWrong](https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is)
