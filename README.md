## againstJustaism

### Primary instances of Justaism

"The widespread enthusiasm for LLMs should be tempered by an awareness that they are not actually simulating human intelligence [29,32,33]. LLMs simply predict the next phrase or sentence, given what they have been exposed to in the training data. Consequently, they tend to output an ‘average’ of what the internet or popular books tend to say"<br>
[Demszky et al., 2023, Nature Reviews Psychology](https://doi.org/10.1038/s44159-023-00241-5)

"A bare-bones LLM does not really know anything because all it does, at a fundamental level, is sequence prediction."<br>
[Shanahan, 2024, Communications of the ACM](https://dx.doi.org/10.1145/3624724)

"As sequence predictors, these models draw on the underlying statistical distribution of previously generated text to stitch together vectorized symbol strings based on the probabilities of their co-occurrence4. They therefore lack the communicatively embodied and relational functionings that are a prerequisite of scientific meaning-making, in the barest sense."<br>
[Birhane et al., 2023, Nature Reviews Physics](https://doi.org/10.1038/s42254-023-00581-4)

"LLMs are not trained to understand language as humans do. By ‘learning’ the statistical associations between words as they have been used by humans, GPT-3 develops an ability to successfully predict which word best completes a phrase or sentence."<br>
[Thirunavukarasu et al., 2023, Nature Medicine](https://doi.org/10.1038/s41591-023-02448-8)

"In our view, they [LLMs] operate over ‘fossilized’ outputs of human language (text token) and seem capable of implementing some automatic computations pertaining to distributional statistics, but are incapable of understanding due to their lack of generative world models"<br>
[Marcus et al., 2023, arXiv](https://doi.org/10.48550/arXiv.2308.00109)

"An AI doesn’t contemplate your question. When you understand how they work, even at a superficial level, you realize it’s just a statistical algorithm. It’s a very cool and impressive statistical algorithm, but it doesn’t think or consider. Some people are surprised to learn that there’s no ‘mind’ there at all."<br>
[Woolridge, 2023, Interview with Elsevier Connect](https://www.elsevier.com/connect/with-the-rise-of-llms-what-should-we-really-be-concerned-about)

"First we should ask the question, have large language models achieved anything, anything in this domain [natural language understanding]. Answer: no, they've achieved zero. [...] GPT-3 has done nothing. With a supercomputer it can look at 45 terabytes of data and find some superficial regularities, which then, it can imitate. "<br>
[Chomsky, 2023, Interview with Machine Learning Street Talk](https://www.youtube.com/watch?v=axuGfh4UR9Q)

"Put simply, LLMs do not understand the prompts given to them, and do not understand the text they generate. From a Linguistics perspective, researchers like
Bender & Koller [12], highlight that the data upon which LLMs are trained does
not contain intent that is present in human communication. This lack of intent
means that only very weak semantics, or meaning, can be captured by a LLM
trained on purely expression (structural) data." <br>
[O'Neil & Connor, 2023, arXiv](https://doi.org/10.48550/arXiv.2307.04821)



### Secondary references to Justaism 

In response to the survey item – "Some generative model trained only on text, given enough data and computational resources, could understand natural
language in some non-trivial sense." – 51% agreed and the remaining 49% disagreed (out of 480 active NLP researcher respondents).<br>
[Michael et al., 2022, arXiv](https://doi.org/10.48550/arXiv.2208.12852)

'there is a small but growing belief that a next-token predicting model is merelyan impressive improv artist that cannot truly model human thought. [...] This criticism has been floating around as an informal viewpoint (LeCun, 2024; Bubeck et al., 2023).'<br>
[Bachmann & Nagarajan, 2024, arXiv](https://doi.org/10.48550/arXiv.2403.06963) 

"I suspect that the general public, including many journalists reporting on machine learning, aren’t even aware of the distinction between training the model and using it to make inferences. One simply reads that ChatGPT, or any other comparable LLM, generates text by predicting the next word. This mis-communication is a MAJOR blunder." <br>
[Benzon, 2023, LessWrong](https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is)
