## againstJustaism

### Primary instances of Justaism

"The widespread enthusiasm for LLMs should be tempered by an awareness that they are not actually simulating human intelligence [29,32,33]. LLMs simply predict the next phrase or sentence, given what they have been exposed to in the training data. Consequently, they tend to output an ‘average’ of what the internet or popular books tend to say"<br>
[Demszky et al., 2023](https://doi.org/10.1038/s44159-023-00241-5)

"A bare-bones LLM does not really know anything because all it does, at a fundamental level, is sequence prediction."<br>
[Shanahan, 2024](https://dx.doi.org/10.1145/3624724)

"As sequence predictors, these models draw on the underlying statistical distribution of previously generated text to stitch together vectorized symbol strings based on the probabilities of their co-occurrence4. They therefore lack the communicatively embodied and relational functionings that are a prerequisite of scientific meaning-making, in the barest sense."<br>
[Birhane et al., 2023](https://doi.org/10.1038/s42254-023-00581-4)

"LLMs are not trained to understand language as humans do. By ‘learning’ the statistical associations between words as they have been used by humans, GPT-3 develops an ability to successfully predict which word best completes a phrase or sentence."<br>
[Thirunavukarasu et al., 2023](https://doi.org/10.1038/s41591-023-02448-8)

"In our view, they [LLMs] operate over ‘fossilized’ outputs of human language (text token) and seem capable of implementing some automatic computations pertaining to distributional statistics, but are incapable of understanding due to their lack of generative world models"<br>
[Marcus et al., 2023](https://doi.org/10.48550/arXiv.2308.00109)"

"An AI doesn’t contemplate your question. When you understand how they work, even at a superficial level, you realize it’s just a statistical algorithm. It’s a very cool and impressive statistical algorithm, but it doesn’t think or consider. Some people are surprised to learn that there’s no ‘mind’ there at all."<br>
[Evans, 2023](https://www.elsevier.com/connect/with-the-rise-of-llms-what-should-we-really-be-concerned-about)

"First we should ask the question, have large language models achieved anything, anything in this domain [natural language understanding]. Answer: no, they've achieved zero."<br>
[Chomsky, 2023](https://www.youtube.com/watch?v=axuGfh4UR9Q)



### Secondary references to Justaism 

In response to the [survey](https://doi.org/10.48550/arXiv.2208.12852) item – "Some generative model trained only on text, given enough data and computational resources, could understand natural
language in some non-trivial sense." – 51% agreed and the remaining 49% disagreed (out of 480 active NLP researcher respondents). 

'there is a small but growing belief that a next-token predicting model is merelyan impressive improv artist that cannot truly model human thought. [...] This criticism has been floating around as an informal viewpoint (LeCun, 2024; Bubeck et al., 2023).'<br>
[Bachmann & Nagarajan, 2024](https://doi.org/10.48550/arXiv.2403.06963) 

"I suspect that the general public, including many journalists reporting on machine learning, aren’t even aware of the distinction between training the model and using it to make inferences. One simply reads that ChatGPT, or any other comparable LLM, generates text by predicting the next word. This mis-communication is a MAJOR blunder." <br>
[Benzon, 2023](https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is)
